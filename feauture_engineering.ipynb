{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import string \n",
    "import seaborn as sns \n",
    "import nltk \n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import os \n",
    "import contractions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings from Exploratory Data Analysis \n",
    "\n",
    "#### 1. Statistically its proven that The number of words in Transcript is different for Selection and Rejection Candidates (Using T-Test) \n",
    "\n",
    "#### 2. Using the Tf-IDF vectorizer , it cames out that only similarity between transcript and Job decription is higher for selected candidates compared with similarity between transcript and resume , resume and Job description \n",
    "\n",
    "#### 3. But when we used the Word to Vector model, the similarity between Resume and Job description, Job description and Transcript , Transcript and Resume are higher for selected candidates compared with the rejected candidates\n",
    "\n",
    "#### 4. data analyst,data engineer,data scientist,product manager,software engineer,ui designer,ui engineer  are the only roles which have only enough data to train on all other roles have data between 10-60 data points, so we dropped them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/InterviewAutomation/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_files = [file for file in os.listdir(path) if file.endswith(('.xls', '.xlsx'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Name', 'Role', 'Transcript', 'Resume', 'decision',\n",
      "       'Reason for decision', 'Job Description'],\n",
      "      dtype='object')\n",
      "Index(['ID', 'Name', 'Role', 'Transcript', 'Resume', 'decision',\n",
      "       'Reason for decision', 'Job Description'],\n",
      "      dtype='object')\n",
      "Index(['ID', 'Name', 'Role', 'Transcript', 'Resume', 'decision',\n",
      "       'Reason for decision', 'Job Description'],\n",
      "      dtype='object')\n",
      "Index(['ID', 'Name', 'Role', 'Transcript', 'Resume', 'decision',\n",
      "       'Reason for decision', 'Job Description'],\n",
      "      dtype='object')\n",
      "Index(['ID', 'Name', 'Role', 'Transcript', 'Resume', 'decision',\n",
      "       'Reason for decision', 'Job Description'],\n",
      "      dtype='object')\n",
      "Index(['ID', 'Name', 'Role', 'Transcript', 'Resume', 'decision',\n",
      "       'Reason for decision', 'Job Description'],\n",
      "      dtype='object')\n",
      "Index(['ID', 'Name', 'Role', 'Transcript', 'Resume', 'decision',\n",
      "       'Reason for decision', 'Job Description'],\n",
      "      dtype='object')\n",
      "Index(['ID', 'Name', 'Role', 'Transcript', 'Resume', 'decision',\n",
      "       'Reason for decision', 'Job Description'],\n",
      "      dtype='object')\n",
      "Index(['ID', 'Name', 'Role', 'Transcript', 'Resume', 'decision',\n",
      "       'Reason for decision', 'Job Description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for file in excel_files:\n",
    "    df = pd.read_excel(path+'/'+file)\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Changing the Columns of the 4th dataset as it does have the Wrong Columns\n",
    "# df = pd.read_excel(excel_files[3])\n",
    "# df.columns = ['ID', 'Name', 'Role', 'Transcript', 'Resume', 'decision',\n",
    "#        'Reason for decision', 'Job Description']\n",
    "# df.to_excel(path+'/'+'dataset4.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in excel_files:\n",
    "#     df = pd.read_excel(path+'/'+file)\n",
    "#     print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Merging all the datasets and creating the full dataset for traing and testing\n",
    "dataframes = []\n",
    "\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(path, file)\n",
    "    df = pd.read_excel(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all dataframes into a single dataframe\n",
    "data = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3250, 8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Role</th>\n",
       "      <th>Transcript</th>\n",
       "      <th>Resume</th>\n",
       "      <th>decision</th>\n",
       "      <th>Reason for decision</th>\n",
       "      <th>Job Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brenbr359</td>\n",
       "      <td>Brent Brown</td>\n",
       "      <td>Product Manager</td>\n",
       "      <td>Product Manager Interview Transcript\\n\\nInterv...</td>\n",
       "      <td>Here's a sample resume for Brent Brown applyin...</td>\n",
       "      <td>select</td>\n",
       "      <td>Experience</td>\n",
       "      <td>We are looking for a skilled Product Manager w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jameay305</td>\n",
       "      <td>James Ayala</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Software Engineer Interview Transcript\\n\\nInte...</td>\n",
       "      <td>Here's a sample resume for James Ayala applyin...</td>\n",
       "      <td>select</td>\n",
       "      <td>Experience</td>\n",
       "      <td>We are looking for a skilled Software Engineer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scotri565</td>\n",
       "      <td>Scott Rivera</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Here is a simulated interview for Scott Rivera...</td>\n",
       "      <td>Here's a sample resume for Scott Rivera applyi...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Experience</td>\n",
       "      <td>We are looking for a skilled Data Engineer wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emilke232</td>\n",
       "      <td>Emily Kelly</td>\n",
       "      <td>UI Engineer</td>\n",
       "      <td>Interview Transcript: Emily Kelly for UI Engin...</td>\n",
       "      <td>Here's a sample resume for Emily Kelly:\\n\\nEmi...</td>\n",
       "      <td>select</td>\n",
       "      <td>Experience</td>\n",
       "      <td>We are looking for a skilled UI Engineer with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ashlra638</td>\n",
       "      <td>Ashley Ray</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist Interview Transcript\\n\\nCompany...</td>\n",
       "      <td>Here's a sample resume for Ashley Ray applying...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Cultural Fit</td>\n",
       "      <td>We are looking for a skilled Data Scientist wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID          Name               Role  \\\n",
       "0  brenbr359   Brent Brown    Product Manager   \n",
       "1  jameay305   James Ayala  Software Engineer   \n",
       "2  scotri565  Scott Rivera      Data Engineer   \n",
       "3  emilke232   Emily Kelly        UI Engineer   \n",
       "4  ashlra638    Ashley Ray     Data Scientist   \n",
       "\n",
       "                                          Transcript  \\\n",
       "0  Product Manager Interview Transcript\\n\\nInterv...   \n",
       "1  Software Engineer Interview Transcript\\n\\nInte...   \n",
       "2  Here is a simulated interview for Scott Rivera...   \n",
       "3  Interview Transcript: Emily Kelly for UI Engin...   \n",
       "4  Data Scientist Interview Transcript\\n\\nCompany...   \n",
       "\n",
       "                                              Resume decision  \\\n",
       "0  Here's a sample resume for Brent Brown applyin...   select   \n",
       "1  Here's a sample resume for James Ayala applyin...   select   \n",
       "2  Here's a sample resume for Scott Rivera applyi...   reject   \n",
       "3  Here's a sample resume for Emily Kelly:\\n\\nEmi...   select   \n",
       "4  Here's a sample resume for Ashley Ray applying...   reject   \n",
       "\n",
       "  Reason for decision                                    Job Description  \n",
       "0          Experience  We are looking for a skilled Product Manager w...  \n",
       "1          Experience  We are looking for a skilled Software Engineer...  \n",
       "2          Experience  We are looking for a skilled Data Engineer wit...  \n",
       "3          Experience  We are looking for a skilled UI Engineer with ...  \n",
       "4        Cultural Fit  We are looking for a skilled Data Scientist wi...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3250 entries, 0 to 3249\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   ID                   3250 non-null   object\n",
      " 1   Name                 3250 non-null   object\n",
      " 2   Role                 3250 non-null   object\n",
      " 3   Transcript           3250 non-null   object\n",
      " 4   Resume               3250 non-null   object\n",
      " 5   decision             3250 non-null   object\n",
      " 6   Reason for decision  3250 non-null   object\n",
      " 7   Job Description      3250 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 203.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    data[col] = data[col].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['select', 'reject', 'rejected', 'selected'], dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['decision'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decision(text):\n",
    "    if text in ['select','selected']:\n",
    "        return 'select'\n",
    "    else :\n",
    "        return 'reject'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['decision'] = data['decision'].apply(process_decision )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['product manager', 'software engineer', 'data engineer',\n",
       "       'ui engineer', 'data scientist', 'data analyst', 'ui designer',\n",
       "       'digital marketing specialist', 'cybersecurity specialist',\n",
       "       'hr specialist', 'network engineer', 'graphic designer',\n",
       "       'game developer', 'mobile app developer', 'cloud architect',\n",
       "       'business analyst', 'database administrator', 'devops engineer',\n",
       "       'machine learning engineer', 'content writer',\n",
       "       'system administrator', 'ui/ux designer', 'ai engineer',\n",
       "       'project manager', 'software developer'], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Role\n",
       "ai engineer                      13\n",
       "business analyst                 11\n",
       "cloud architect                   8\n",
       "content writer                    5\n",
       "cybersecurity specialist          9\n",
       "data analyst                    267\n",
       "data engineer                   509\n",
       "data scientist                  592\n",
       "database administrator            9\n",
       "devops engineer                  16\n",
       "digital marketing specialist     14\n",
       "game developer                   10\n",
       "graphic designer                 15\n",
       "hr specialist                    12\n",
       "machine learning engineer        13\n",
       "mobile app developer              7\n",
       "network engineer                 16\n",
       "product manager                 514\n",
       "project manager                  56\n",
       "software developer               66\n",
       "software engineer               551\n",
       "system administrator              5\n",
       "ui designer                     234\n",
       "ui engineer                     291\n",
       "ui/ux designer                    7\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_count = data.groupby('Role')['ID'].count()\n",
    "unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping Rows which have count < 100 \n",
    "for count , role in zip(unique_count.values, unique_count.index):\n",
    "    if count < 100 :\n",
    "        data = data[data['Role'] != role] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Role\n",
       "data analyst         267\n",
       "data engineer        509\n",
       "data scientist       592\n",
       "product manager      514\n",
       "software engineer    551\n",
       "ui designer          234\n",
       "ui engineer          291\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_count = data.groupby('Role')['ID'].count()\n",
    "unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958, 8)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Performing Feature Engineering\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "stopword_s = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def textpreprocessing(text):\n",
    "    text = contractions.fix(text) # expanding the contractions like , I'm with I am and it's with It is and so on...\n",
    "    text = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@\\[\\]^_`{|}~]', '', text)  # Removing all these characters from the text \n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)   # Splitting the text as sentences using the regular expressions     \n",
    "    for idx, sent in enumerate(sentences):\n",
    "        words = nlp(sent)  # converting the sentence into words\n",
    "        words = [word.text for word in words if word.text.lower() not in stopword_s] # Removing the stopwords from the text\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]  # Using the Lemmatization techniques\n",
    "        sentences[idx] = ' '.join(words)  # Replacing the original sentence with the new sentence \n",
    "    return ' '.join(sentences)  # Join the sentences back together  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Transcript_processed'] = data['Transcript'].apply(textpreprocessing)\n",
    "# Saving the processed Data to a temporary file\n",
    "data.to_excel(path+'/'+'final_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Resume_processed'] = data['Resume'].apply(textpreprocessing)\n",
    "# Saving the processed Data to a temporary file\n",
    "data.to_excel(path+'/'+'final_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Job_Description_processed'] = data['Job Description'].apply(textpreprocessing)\n",
    "# Saving the processed Data to a temporary file\n",
    "data.to_excel(path+'/'+'final_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_final = \"D:/InterviewAutomation/data/final_data.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(path_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Role</th>\n",
       "      <th>Transcript</th>\n",
       "      <th>Resume</th>\n",
       "      <th>decision</th>\n",
       "      <th>Reason for decision</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Transcript_processed</th>\n",
       "      <th>Resume_processed</th>\n",
       "      <th>Job_Description_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>brenbr359</td>\n",
       "      <td>brent brown</td>\n",
       "      <td>product manager</td>\n",
       "      <td>product manager interview transcript\\n\\ninterv...</td>\n",
       "      <td>here's a sample resume for brent brown applyin...</td>\n",
       "      <td>select</td>\n",
       "      <td>experience</td>\n",
       "      <td>we are looking for a skilled product manager w...</td>\n",
       "      <td>product manager interview transcript \\n\\n inte...</td>\n",
       "      <td>sample resume brent brown applying role produc...</td>\n",
       "      <td>looking skilled product manager expertise data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>jameay305</td>\n",
       "      <td>james ayala</td>\n",
       "      <td>software engineer</td>\n",
       "      <td>software engineer interview transcript\\n\\ninte...</td>\n",
       "      <td>here's a sample resume for james ayala applyin...</td>\n",
       "      <td>select</td>\n",
       "      <td>experience</td>\n",
       "      <td>we are looking for a skilled software engineer...</td>\n",
       "      <td>software engineer interview transcript \\n\\n in...</td>\n",
       "      <td>sample resume james ayala applying software en...</td>\n",
       "      <td>looking skilled software engineer expertise py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>scotri565</td>\n",
       "      <td>scott rivera</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>here is a simulated interview for scott rivera...</td>\n",
       "      <td>here's a sample resume for scott rivera applyi...</td>\n",
       "      <td>reject</td>\n",
       "      <td>experience</td>\n",
       "      <td>we are looking for a skilled data engineer wit...</td>\n",
       "      <td>simulated interview scott rivera applying role...</td>\n",
       "      <td>sample resume scott rivera applying role data ...</td>\n",
       "      <td>looking skilled data engineer expertise cloud ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>emilke232</td>\n",
       "      <td>emily kelly</td>\n",
       "      <td>ui engineer</td>\n",
       "      <td>interview transcript: emily kelly for ui engin...</td>\n",
       "      <td>here's a sample resume for emily kelly:\\n\\nemi...</td>\n",
       "      <td>select</td>\n",
       "      <td>experience</td>\n",
       "      <td>we are looking for a skilled ui engineer with ...</td>\n",
       "      <td>interview transcript emily kelly ui engineer p...</td>\n",
       "      <td>sample resume emily kelly \\n\\n emily kelly \\n ...</td>\n",
       "      <td>looking skilled ui engineer expertise uiux des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ashlra638</td>\n",
       "      <td>ashley ray</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>data scientist interview transcript\\n\\ncompany...</td>\n",
       "      <td>here's a sample resume for ashley ray applying...</td>\n",
       "      <td>reject</td>\n",
       "      <td>cultural fit</td>\n",
       "      <td>we are looking for a skilled data scientist wi...</td>\n",
       "      <td>data scientist interview transcript \\n\\n compa...</td>\n",
       "      <td>sample resume ashley ray applying data scienti...</td>\n",
       "      <td>looking skilled data scientist expertise sql m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         ID          Name               Role  \\\n",
       "0           0  brenbr359   brent brown    product manager   \n",
       "1           1  jameay305   james ayala  software engineer   \n",
       "2           2  scotri565  scott rivera      data engineer   \n",
       "3           3  emilke232   emily kelly        ui engineer   \n",
       "4           4  ashlra638    ashley ray     data scientist   \n",
       "\n",
       "                                          Transcript  \\\n",
       "0  product manager interview transcript\\n\\ninterv...   \n",
       "1  software engineer interview transcript\\n\\ninte...   \n",
       "2  here is a simulated interview for scott rivera...   \n",
       "3  interview transcript: emily kelly for ui engin...   \n",
       "4  data scientist interview transcript\\n\\ncompany...   \n",
       "\n",
       "                                              Resume decision  \\\n",
       "0  here's a sample resume for brent brown applyin...   select   \n",
       "1  here's a sample resume for james ayala applyin...   select   \n",
       "2  here's a sample resume for scott rivera applyi...   reject   \n",
       "3  here's a sample resume for emily kelly:\\n\\nemi...   select   \n",
       "4  here's a sample resume for ashley ray applying...   reject   \n",
       "\n",
       "  Reason for decision                                    Job Description  \\\n",
       "0          experience  we are looking for a skilled product manager w...   \n",
       "1          experience  we are looking for a skilled software engineer...   \n",
       "2          experience  we are looking for a skilled data engineer wit...   \n",
       "3          experience  we are looking for a skilled ui engineer with ...   \n",
       "4        cultural fit  we are looking for a skilled data scientist wi...   \n",
       "\n",
       "                                Transcript_processed  \\\n",
       "0  product manager interview transcript \\n\\n inte...   \n",
       "1  software engineer interview transcript \\n\\n in...   \n",
       "2  simulated interview scott rivera applying role...   \n",
       "3  interview transcript emily kelly ui engineer p...   \n",
       "4  data scientist interview transcript \\n\\n compa...   \n",
       "\n",
       "                                    Resume_processed  \\\n",
       "0  sample resume brent brown applying role produc...   \n",
       "1  sample resume james ayala applying software en...   \n",
       "2  sample resume scott rivera applying role data ...   \n",
       "3  sample resume emily kelly \\n\\n emily kelly \\n ...   \n",
       "4  sample resume ashley ray applying data scienti...   \n",
       "\n",
       "                           Job_Description_processed  \n",
       "0  looking skilled product manager expertise data...  \n",
       "1  looking skilled software engineer expertise py...  \n",
       "2  looking skilled data engineer expertise cloud ...  \n",
       "3  looking skilled ui engineer expertise uiux des...  \n",
       "4  looking skilled data scientist expertise sql m...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.drop(columns=['Unnamed: 0', 'Transcript' , 'Resume' , 'Job Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Role</th>\n",
       "      <th>decision</th>\n",
       "      <th>Reason for decision</th>\n",
       "      <th>Transcript_processed</th>\n",
       "      <th>Resume_processed</th>\n",
       "      <th>Job_Description_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brenbr359</td>\n",
       "      <td>brent brown</td>\n",
       "      <td>product manager</td>\n",
       "      <td>select</td>\n",
       "      <td>experience</td>\n",
       "      <td>product manager interview transcript \\n\\n inte...</td>\n",
       "      <td>sample resume brent brown applying role produc...</td>\n",
       "      <td>looking skilled product manager expertise data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jameay305</td>\n",
       "      <td>james ayala</td>\n",
       "      <td>software engineer</td>\n",
       "      <td>select</td>\n",
       "      <td>experience</td>\n",
       "      <td>software engineer interview transcript \\n\\n in...</td>\n",
       "      <td>sample resume james ayala applying software en...</td>\n",
       "      <td>looking skilled software engineer expertise py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scotri565</td>\n",
       "      <td>scott rivera</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>reject</td>\n",
       "      <td>experience</td>\n",
       "      <td>simulated interview scott rivera applying role...</td>\n",
       "      <td>sample resume scott rivera applying role data ...</td>\n",
       "      <td>looking skilled data engineer expertise cloud ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emilke232</td>\n",
       "      <td>emily kelly</td>\n",
       "      <td>ui engineer</td>\n",
       "      <td>select</td>\n",
       "      <td>experience</td>\n",
       "      <td>interview transcript emily kelly ui engineer p...</td>\n",
       "      <td>sample resume emily kelly \\n\\n emily kelly \\n ...</td>\n",
       "      <td>looking skilled ui engineer expertise uiux des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ashlra638</td>\n",
       "      <td>ashley ray</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>reject</td>\n",
       "      <td>cultural fit</td>\n",
       "      <td>data scientist interview transcript \\n\\n compa...</td>\n",
       "      <td>sample resume ashley ray applying data scienti...</td>\n",
       "      <td>looking skilled data scientist expertise sql m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>kotasai181</td>\n",
       "      <td>hyma garg</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>select</td>\n",
       "      <td>excellent communication skills, strong technic...</td>\n",
       "      <td>interviewer dr rachel kim data science lead \\n...</td>\n",
       "      <td>detailed dummy resume hyma garg \\n\\n hyma garg...</td>\n",
       "      <td>expectedexperience   35 year domain data analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>kotasai183</td>\n",
       "      <td>eshani sharma</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>reject</td>\n",
       "      <td>low problem-solving abilities, not enough tech...</td>\n",
       "      <td>interview transcript eshani sharma   data scie...</td>\n",
       "      <td>detailed dummy resume eshani sharma \\n\\n eshan...</td>\n",
       "      <td>expectedexperience   02 year domain data analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>kotasai184</td>\n",
       "      <td>palash kumar</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>reject</td>\n",
       "      <td>low problem-solving abilities, inability to co...</td>\n",
       "      <td>transcript interview palash kumar data scienti...</td>\n",
       "      <td>palash kumar \\n contact information \\n\\n  emai...</td>\n",
       "      <td>expectedexperience   35 year domain data analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>kotasai186</td>\n",
       "      <td>kashvi das</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>reject</td>\n",
       "      <td>unsuitable for the job role, not enough techni...</td>\n",
       "      <td>interview transcript kashvi da data scientist ...</td>\n",
       "      <td>detailed dummy resume kashvi da \\n\\n contact i...</td>\n",
       "      <td>expectedexperience   68 year domain data analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>kotasai190</td>\n",
       "      <td>forum bhardwaj</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>reject</td>\n",
       "      <td>not enough technical expertise, lack of releva...</td>\n",
       "      <td>interview transcript forum bhardwaj   data sci...</td>\n",
       "      <td>detailed dummy resume candidate forum bhardwaj...</td>\n",
       "      <td>expectedexperience   02 year domain data analy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2958 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID            Name               Role decision  \\\n",
       "0      brenbr359     brent brown    product manager   select   \n",
       "1      jameay305     james ayala  software engineer   select   \n",
       "2      scotri565    scott rivera      data engineer   reject   \n",
       "3      emilke232     emily kelly        ui engineer   select   \n",
       "4      ashlra638      ashley ray     data scientist   reject   \n",
       "...          ...             ...                ...      ...   \n",
       "2953  kotasai181       hyma garg     data scientist   select   \n",
       "2954  kotasai183   eshani sharma     data scientist   reject   \n",
       "2955  kotasai184    palash kumar     data scientist   reject   \n",
       "2956  kotasai186      kashvi das     data scientist   reject   \n",
       "2957  kotasai190  forum bhardwaj     data scientist   reject   \n",
       "\n",
       "                                    Reason for decision  \\\n",
       "0                                            experience   \n",
       "1                                            experience   \n",
       "2                                            experience   \n",
       "3                                            experience   \n",
       "4                                          cultural fit   \n",
       "...                                                 ...   \n",
       "2953  excellent communication skills, strong technic...   \n",
       "2954  low problem-solving abilities, not enough tech...   \n",
       "2955  low problem-solving abilities, inability to co...   \n",
       "2956  unsuitable for the job role, not enough techni...   \n",
       "2957  not enough technical expertise, lack of releva...   \n",
       "\n",
       "                                   Transcript_processed  \\\n",
       "0     product manager interview transcript \\n\\n inte...   \n",
       "1     software engineer interview transcript \\n\\n in...   \n",
       "2     simulated interview scott rivera applying role...   \n",
       "3     interview transcript emily kelly ui engineer p...   \n",
       "4     data scientist interview transcript \\n\\n compa...   \n",
       "...                                                 ...   \n",
       "2953  interviewer dr rachel kim data science lead \\n...   \n",
       "2954  interview transcript eshani sharma   data scie...   \n",
       "2955  transcript interview palash kumar data scienti...   \n",
       "2956  interview transcript kashvi da data scientist ...   \n",
       "2957  interview transcript forum bhardwaj   data sci...   \n",
       "\n",
       "                                       Resume_processed  \\\n",
       "0     sample resume brent brown applying role produc...   \n",
       "1     sample resume james ayala applying software en...   \n",
       "2     sample resume scott rivera applying role data ...   \n",
       "3     sample resume emily kelly \\n\\n emily kelly \\n ...   \n",
       "4     sample resume ashley ray applying data scienti...   \n",
       "...                                                 ...   \n",
       "2953  detailed dummy resume hyma garg \\n\\n hyma garg...   \n",
       "2954  detailed dummy resume eshani sharma \\n\\n eshan...   \n",
       "2955  palash kumar \\n contact information \\n\\n  emai...   \n",
       "2956  detailed dummy resume kashvi da \\n\\n contact i...   \n",
       "2957  detailed dummy resume candidate forum bhardwaj...   \n",
       "\n",
       "                              Job_Description_processed  \n",
       "0     looking skilled product manager expertise data...  \n",
       "1     looking skilled software engineer expertise py...  \n",
       "2     looking skilled data engineer expertise cloud ...  \n",
       "3     looking skilled ui engineer expertise uiux des...  \n",
       "4     looking skilled data scientist expertise sql m...  \n",
       "...                                                 ...  \n",
       "2953  expectedexperience   35 year domain data analy...  \n",
       "2954  expectedexperience   02 year domain data analy...  \n",
       "2955  expectedexperience   35 year domain data analy...  \n",
       "2956  expectedexperience   68 year domain data analy...  \n",
       "2957  expectedexperience   02 year domain data analy...  \n",
       "\n",
       "[2958 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Transcript_words'] = new_df['Transcript_processed'].apply(lambda x: len(x.split())) \n",
    "new_df['Transcript_length'] = new_df['Transcript_processed'].apply(lambda x:len(x))\n",
    "\n",
    "new_df['Resume_words'] = new_df['Resume_processed'].apply(lambda x: len(x.split())) \n",
    "new_df['Resume_length'] = new_df['Resume_processed'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Role</th>\n",
       "      <th>decision</th>\n",
       "      <th>Reason for decision</th>\n",
       "      <th>Transcript_processed</th>\n",
       "      <th>Resume_processed</th>\n",
       "      <th>Job_Description_processed</th>\n",
       "      <th>Transcript_words</th>\n",
       "      <th>Transcript_length</th>\n",
       "      <th>Resume_words</th>\n",
       "      <th>Resume_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brenbr359</td>\n",
       "      <td>brent brown</td>\n",
       "      <td>product manager</td>\n",
       "      <td>select</td>\n",
       "      <td>experience</td>\n",
       "      <td>product manager interview transcript \\n\\n inte...</td>\n",
       "      <td>sample resume brent brown applying role produc...</td>\n",
       "      <td>looking skilled product manager expertise data...</td>\n",
       "      <td>348</td>\n",
       "      <td>2537</td>\n",
       "      <td>221</td>\n",
       "      <td>2040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jameay305</td>\n",
       "      <td>james ayala</td>\n",
       "      <td>software engineer</td>\n",
       "      <td>select</td>\n",
       "      <td>experience</td>\n",
       "      <td>software engineer interview transcript \\n\\n in...</td>\n",
       "      <td>sample resume james ayala applying software en...</td>\n",
       "      <td>looking skilled software engineer expertise py...</td>\n",
       "      <td>387</td>\n",
       "      <td>2974</td>\n",
       "      <td>241</td>\n",
       "      <td>2231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scotri565</td>\n",
       "      <td>scott rivera</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>reject</td>\n",
       "      <td>experience</td>\n",
       "      <td>simulated interview scott rivera applying role...</td>\n",
       "      <td>sample resume scott rivera applying role data ...</td>\n",
       "      <td>looking skilled data engineer expertise cloud ...</td>\n",
       "      <td>282</td>\n",
       "      <td>1989</td>\n",
       "      <td>255</td>\n",
       "      <td>2183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emilke232</td>\n",
       "      <td>emily kelly</td>\n",
       "      <td>ui engineer</td>\n",
       "      <td>select</td>\n",
       "      <td>experience</td>\n",
       "      <td>interview transcript emily kelly ui engineer p...</td>\n",
       "      <td>sample resume emily kelly \\n\\n emily kelly \\n ...</td>\n",
       "      <td>looking skilled ui engineer expertise uiux des...</td>\n",
       "      <td>402</td>\n",
       "      <td>2836</td>\n",
       "      <td>237</td>\n",
       "      <td>2035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ashlra638</td>\n",
       "      <td>ashley ray</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>reject</td>\n",
       "      <td>cultural fit</td>\n",
       "      <td>data scientist interview transcript \\n\\n compa...</td>\n",
       "      <td>sample resume ashley ray applying data scienti...</td>\n",
       "      <td>looking skilled data scientist expertise sql m...</td>\n",
       "      <td>306</td>\n",
       "      <td>2187</td>\n",
       "      <td>299</td>\n",
       "      <td>2632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID          Name               Role decision Reason for decision  \\\n",
       "0  brenbr359   brent brown    product manager   select          experience   \n",
       "1  jameay305   james ayala  software engineer   select          experience   \n",
       "2  scotri565  scott rivera      data engineer   reject          experience   \n",
       "3  emilke232   emily kelly        ui engineer   select          experience   \n",
       "4  ashlra638    ashley ray     data scientist   reject        cultural fit   \n",
       "\n",
       "                                Transcript_processed  \\\n",
       "0  product manager interview transcript \\n\\n inte...   \n",
       "1  software engineer interview transcript \\n\\n in...   \n",
       "2  simulated interview scott rivera applying role...   \n",
       "3  interview transcript emily kelly ui engineer p...   \n",
       "4  data scientist interview transcript \\n\\n compa...   \n",
       "\n",
       "                                    Resume_processed  \\\n",
       "0  sample resume brent brown applying role produc...   \n",
       "1  sample resume james ayala applying software en...   \n",
       "2  sample resume scott rivera applying role data ...   \n",
       "3  sample resume emily kelly \\n\\n emily kelly \\n ...   \n",
       "4  sample resume ashley ray applying data scienti...   \n",
       "\n",
       "                           Job_Description_processed  Transcript_words  \\\n",
       "0  looking skilled product manager expertise data...               348   \n",
       "1  looking skilled software engineer expertise py...               387   \n",
       "2  looking skilled data engineer expertise cloud ...               282   \n",
       "3  looking skilled ui engineer expertise uiux des...               402   \n",
       "4  looking skilled data scientist expertise sql m...               306   \n",
       "\n",
       "   Transcript_length  Resume_words  Resume_length  \n",
       "0               2537           221           2040  \n",
       "1               2974           241           2231  \n",
       "2               1989           255           2183  \n",
       "3               2836           237           2035  \n",
       "4               2187           299           2632  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_words_role = new_df.groupby(['Role','decision'])['Transcript_words'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Role               decision\n",
       "data analyst       reject      376.007194\n",
       "                   select      440.632812\n",
       "data engineer      reject      354.907407\n",
       "                   select      428.108787\n",
       "data scientist     reject      365.095395\n",
       "                   select      420.697917\n",
       "product manager    reject      341.482759\n",
       "                   select      418.569170\n",
       "software engineer  reject      346.182143\n",
       "                   select      400.767528\n",
       "ui designer        reject      383.600000\n",
       "                   select      432.403670\n",
       "ui engineer        reject      314.534722\n",
       "                   select      374.360544\n",
       "Name: Transcript_words, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_words_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistical Analysis for checking wether there is a significant difference between the number of words in Transcript, for select and reject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null Hypothesis : The mean number of words in transcript for the decision of rejection and selection are equal \n",
    "#### Alternative hypothesis : The mean number of words in transcript for the decision of rejection and selection are not equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decision\n",
       "reject    353.499672\n",
       "select    415.713589\n",
       "Name: Transcript_words, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.groupby(['decision'])['Transcript_words'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = new_df.groupby(['decision'])['Transcript_words'].mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_sample = new_df[new_df['decision'] == 'reject'].sample(300)\n",
    "select_sample = new_df[new_df['decision'] == 'select'].sample(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_var = (reject_sample['Transcript_words'].std())**2 \n",
    "select_var = (select_sample['Transcript_words'].std())**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8821574140776041"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reject_var/select_var  # This is in between 0.5 and 2, so we need to take the variances as almost equal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: -7.778672124809383\n",
      "P-value: 3.2450284968961304e-14\n"
     ]
    }
   ],
   "source": [
    "# Using T-distribution for testing \n",
    "# Sampling the original distribution \n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "\n",
    "\n",
    "t_stat, p_value = ttest_ind(\n",
    "    reject_sample['Transcript_words'], \n",
    "    select_sample['Transcript_words'], \n",
    "    equal_var=False # Use Welch's t-test if variances are unequal\n",
    ")\n",
    "\n",
    "print(\"T-statistic:\", t_stat)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From this we get to know that , there is a significant evidence to reject Null hypothesis that the average number of words for select is equal to the rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def T_test(role , reject_sample,select_sample):\n",
    "    t_stat, p_value = ttest_ind(\n",
    "        reject_sample['Transcript_words'], \n",
    "        select_sample['Transcript_words'], \n",
    "        equal_var=False  # Use Welch's t-test if variances are unequal\n",
    "    )\n",
    "\n",
    "    if p_value < 0.05 :\n",
    "        print(\"Rejecting Null Hypothesis , Mean number of Transcript words for rejection and selection are significantly different for the role of {}\".format(role))\n",
    "    else : \n",
    "        print(\"Accepting Null Hypothesis , Mean number of Transcript words for rejection and selection are equal for the role of {}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def role_wise_test(roles):\n",
    "    for role in roles:\n",
    "        curr_df = new_df[new_df['Role'] == role] \n",
    "        reject_sample = curr_df[curr_df['decision'] == 'reject'].sample(100)\n",
    "        select_sample = curr_df[curr_df['decision'] == 'select'].sample(100) \n",
    "        T_test(role , reject_sample , select_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejecting Null Hypothesis , Mean number of Transcript words for rejection and selection are significantly different for the role of product manager\n",
      "Rejecting Null Hypothesis , Mean number of Transcript words for rejection and selection are significantly different for the role of software engineer\n",
      "Rejecting Null Hypothesis , Mean number of Transcript words for rejection and selection are significantly different for the role of data engineer\n",
      "Rejecting Null Hypothesis , Mean number of Transcript words for rejection and selection are significantly different for the role of ui engineer\n",
      "Rejecting Null Hypothesis , Mean number of Transcript words for rejection and selection are significantly different for the role of data scientist\n",
      "Rejecting Null Hypothesis , Mean number of Transcript words for rejection and selection are significantly different for the role of data analyst\n",
      "Rejecting Null Hypothesis , Mean number of Transcript words for rejection and selection are significantly different for the role of ui designer\n"
     ]
    }
   ],
   "source": [
    "role_wise_test(new_df['Role'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From this we know that, for every role there is a significant difference between the average number of words for selection and rejection for every role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Role\n",
       "data analyst         286.385768\n",
       "data engineer        295.504912\n",
       "data scientist       290.685811\n",
       "product manager      273.021401\n",
       "software engineer    264.444646\n",
       "ui designer          285.034188\n",
       "ui engineer          251.989691\n",
       "Name: Resume_words, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.groupby(['Role'])['Resume_words'].mean() # From this the average number of words in resume for every role are almost equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def T_test1(role , reject_sample,select_sample):\n",
    "    t_stat, p_value = ttest_ind(\n",
    "        reject_sample['Transcript_length'], \n",
    "        select_sample['Transcript_length'], \n",
    "        equal_var=False  # Use Welch's t-test if variances are unequal\n",
    "    )\n",
    "\n",
    "    if p_value < 0.05 :\n",
    "        print(\"Rejecting Null Hypothesis , Mean length of Transcript for rejection and selection are significantly different for the role of {}\".format(role))\n",
    "    else : \n",
    "        print(\"Accepting Null Hypothesis , Mean length of Transcript for rejection and selection are equal for the role of {}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Role               decision\n",
       "data analyst       reject      2818.964029\n",
       "                   select      3456.015625\n",
       "data engineer      reject      2661.629630\n",
       "                   select      3280.334728\n",
       "data scientist     reject      2726.845395\n",
       "                   select      3253.274306\n",
       "product manager    reject      2627.183908\n",
       "                   select      3299.450593\n",
       "software engineer  reject      2602.950000\n",
       "                   select      3123.605166\n",
       "ui designer        reject      2877.280000\n",
       "                   select      3352.669725\n",
       "ui engineer        reject      2288.125000\n",
       "                   select      2807.748299\n",
       "Name: Transcript_length, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.groupby(['Role','decision'])['Transcript_length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def role_wise_test1(roles):\n",
    "    for role in roles:\n",
    "        curr_df = new_df[new_df['Role'] == role] \n",
    "        reject_sample = curr_df[curr_df['decision'] == 'reject'].sample(100)\n",
    "        select_sample = curr_df[curr_df['decision'] == 'select'].sample(100) \n",
    "        T_test1(role , reject_sample , select_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejecting Null Hypothesis , Mean length of Transcript for rejection and selection are significantly different for the role of product manager\n",
      "Rejecting Null Hypothesis , Mean length of Transcript for rejection and selection are significantly different for the role of software engineer\n",
      "Rejecting Null Hypothesis , Mean length of Transcript for rejection and selection are significantly different for the role of data engineer\n",
      "Rejecting Null Hypothesis , Mean length of Transcript for rejection and selection are significantly different for the role of ui engineer\n",
      "Rejecting Null Hypothesis , Mean length of Transcript for rejection and selection are significantly different for the role of data scientist\n",
      "Rejecting Null Hypothesis , Mean length of Transcript for rejection and selection are significantly different for the role of data analyst\n",
      "Rejecting Null Hypothesis , Mean length of Transcript for rejection and selection are significantly different for the role of ui designer\n"
     ]
    }
   ],
   "source": [
    "role_wise_test1(new_df['Role'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding wether there is a similarity between the two transcripts if so exists, we will drop one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Transcripts Dictionary (â‰¥ 90% similarity):\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "## Finding wether there is a similarity between the two transcripts if so exists and we will drop the other transcripts \n",
    "## For 90% similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(new_df['Transcript_processed'])\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Find transcript pairs with similarity >= 0.9\n",
    "threshold = 0.9\n",
    "similar_dict = {}\n",
    "\n",
    "for i in range(cosine_sim.shape[0]):\n",
    "    similar_indices = []\n",
    "    for j in range(i + 1, cosine_sim.shape[1]):  # Avoid redundant comparisons\n",
    "        if cosine_sim[i, j] >= threshold:\n",
    "            similar_indices.append(j)\n",
    "    if similar_indices:\n",
    "        similar_dict[i] = similar_indices\n",
    "\n",
    "# Print the dictionary\n",
    "print(\"Similar Transcripts Dictionary (â‰¥ 90% similarity):\")\n",
    "print(similar_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Transcripts Dictionary (â‰¥ 80% similarity):\n",
      "{600: [1478], 672: [932], 716: [1080], 745: [1017], 774: [992, 1494], 992: [1494], 1018: [1397], 1026: [2890], 1170: [1389, 1407], 1849: [1972], 1897: [1909], 1968: [1985], 2450: [2514, 2551], 2464: [2597, 2626], 2468: [2626], 2473: [2579], 2476: [2572], 2482: [2535], 2492: [2601], 2497: [2614], 2510: [2589], 2514: [2551], 2519: [2534], 2525: [2541, 2565], 2597: [2626], 2606: [2611], 2659: [2774], 2661: [2778], 2667: [2739], 2673: [2725], 2678: [2728, 2828], 2679: [2760, 2789], 2711: [2754], 2714: [2766, 2805], 2740: [2764, 2849], 2743: [2812], 2750: [2812], 2760: [2789], 2766: [2805], 2767: [2790]}\n"
     ]
    }
   ],
   "source": [
    "## Finding wether there is a similarity between the two transcripts if so exists and we will drop the other transcripts \n",
    "## For 80% similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(new_df['Transcript_processed'])\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Find transcript pairs with similarity >= 0.8\n",
    "threshold = 0.8\n",
    "similar_dict = {}\n",
    "\n",
    "for i in range(cosine_sim.shape[0]):\n",
    "    similar_indices = []\n",
    "    for j in range(i + 1, cosine_sim.shape[1]):  # Avoid redundant comparisons\n",
    "        if cosine_sim[i, j] >= threshold:\n",
    "            similar_indices.append(j)\n",
    "    if similar_indices:\n",
    "        similar_dict[i] = similar_indices\n",
    "\n",
    "# Print the dictionary\n",
    "print(\"Similar Transcripts Dictionary (â‰¥ 80% similarity):\")\n",
    "print(similar_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer1 = TfidfVectorizer()\n",
    "transcript_matrix = tfidf_vectorizer1.fit_transform(new_df['Transcript_processed'])\n",
    "\n",
    "tfidf_vectorizer2 = TfidfVectorizer()\n",
    "resume_matrix = tfidf_vectorizer2.fit_transform(new_df['Resume_processed'])\n",
    "\n",
    "tfidf_vectorizer3 = TfidfVectorizer()\n",
    "jobDescription_matrix = tfidf_vectorizer3.fit_transform(new_df['Job_Description_processed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a single TfidfVectorizer for both Resume and Job Description\n",
    "tfidf_vectorizer4 = TfidfVectorizer()\n",
    "combined_text = new_df['Resume_processed'] + \" \" + new_df['Job_Description_processed']\n",
    "tfidf_vectorizer4.fit(combined_text)\n",
    "\n",
    "# Split the combined TF-IDF matrix back into Resume and Job Description matrices\n",
    "resume_matrix = tfidf_vectorizer4.transform(new_df['Resume_processed'])\n",
    "jobDescription_matrix = tfidf_vectorizer4.transform(new_df['Job_Description_processed'])\n",
    "\n",
    "# Calculate row-wise cosine similarity\n",
    "cos_rjd = []\n",
    "for i in range(new_df.shape[0]):\n",
    "    cos_rjd.append(cosine_similarity(resume_matrix[i], jobDescription_matrix[i])[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Resume_JD'] = cos_rjd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a single TfidfVectorizer for both Resume and Transcript\n",
    "tfidf_vectorizer5 = TfidfVectorizer()\n",
    "combined_text = new_df['Resume_processed'] + \" \" + new_df['Transcript_processed']\n",
    "tfidf_vectorizer5.fit(combined_text)\n",
    "\n",
    "# Split the combined TF-IDF matrix back into Resume and Job Description matrices\n",
    "resume_matrix = tfidf_vectorizer5.transform(new_df['Resume_processed'])\n",
    "Transcript_matrix = tfidf_vectorizer5.transform(new_df['Transcript_processed'])\n",
    "\n",
    "# Calculate row-wise cosine similarity\n",
    "cos_rt = []\n",
    "for i in range(new_df.shape[0]):\n",
    "    cos_rt.append(cosine_similarity(resume_matrix[i], Transcript_matrix[i])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Resume_Transcript'] = cos_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a single TfidfVectorizer for both Job description and Transcript\n",
    "tfidf_vectorizer6 = TfidfVectorizer()\n",
    "combined_text = new_df['Job_Description_processed'] + \" \" + new_df['Transcript_processed']\n",
    "tfidf_vectorizer6.fit(combined_text)\n",
    "\n",
    "# Split the combined TF-IDF matrix back into Resume and Job Description matrices\n",
    "JD_matrix = tfidf_vectorizer6.transform(new_df['Job_Description_processed'])\n",
    "Transcript_matrix = tfidf_vectorizer6.transform(new_df['Transcript_processed'])\n",
    "\n",
    "# Calculate row-wise cosine similarity\n",
    "cos_jdt = []\n",
    "for i in range(new_df.shape[0]):\n",
    "    cos_jdt.append(cosine_similarity(JD_matrix[i], Transcript_matrix[i])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['JobDescription_Transcript'] = cos_jdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Role</th>\n",
       "      <th>decision</th>\n",
       "      <th>Reason for decision</th>\n",
       "      <th>Transcript_processed</th>\n",
       "      <th>Resume_processed</th>\n",
       "      <th>Job_Description_processed</th>\n",
       "      <th>Transcript_words</th>\n",
       "      <th>Transcript_length</th>\n",
       "      <th>Resume_words</th>\n",
       "      <th>Resume_length</th>\n",
       "      <th>Resume_JD</th>\n",
       "      <th>Resume_Transcript</th>\n",
       "      <th>JobDescription_Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brenbr359</td>\n",
       "      <td>brent brown</td>\n",
       "      <td>product manager</td>\n",
       "      <td>select</td>\n",
       "      <td>experience</td>\n",
       "      <td>product manager interview transcript \\n\\n inte...</td>\n",
       "      <td>sample resume brent brown applying role produc...</td>\n",
       "      <td>looking skilled product manager expertise data...</td>\n",
       "      <td>348</td>\n",
       "      <td>2537</td>\n",
       "      <td>221</td>\n",
       "      <td>2040</td>\n",
       "      <td>0.351376</td>\n",
       "      <td>0.389509</td>\n",
       "      <td>0.151835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jameay305</td>\n",
       "      <td>james ayala</td>\n",
       "      <td>software engineer</td>\n",
       "      <td>select</td>\n",
       "      <td>experience</td>\n",
       "      <td>software engineer interview transcript \\n\\n in...</td>\n",
       "      <td>sample resume james ayala applying software en...</td>\n",
       "      <td>looking skilled software engineer expertise py...</td>\n",
       "      <td>387</td>\n",
       "      <td>2974</td>\n",
       "      <td>241</td>\n",
       "      <td>2231</td>\n",
       "      <td>0.272022</td>\n",
       "      <td>0.380939</td>\n",
       "      <td>0.091687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scotri565</td>\n",
       "      <td>scott rivera</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>reject</td>\n",
       "      <td>experience</td>\n",
       "      <td>simulated interview scott rivera applying role...</td>\n",
       "      <td>sample resume scott rivera applying role data ...</td>\n",
       "      <td>looking skilled data engineer expertise cloud ...</td>\n",
       "      <td>282</td>\n",
       "      <td>1989</td>\n",
       "      <td>255</td>\n",
       "      <td>2183</td>\n",
       "      <td>0.465514</td>\n",
       "      <td>0.384318</td>\n",
       "      <td>0.077372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emilke232</td>\n",
       "      <td>emily kelly</td>\n",
       "      <td>ui engineer</td>\n",
       "      <td>select</td>\n",
       "      <td>experience</td>\n",
       "      <td>interview transcript emily kelly ui engineer p...</td>\n",
       "      <td>sample resume emily kelly \\n\\n emily kelly \\n ...</td>\n",
       "      <td>looking skilled ui engineer expertise uiux des...</td>\n",
       "      <td>402</td>\n",
       "      <td>2836</td>\n",
       "      <td>237</td>\n",
       "      <td>2035</td>\n",
       "      <td>0.313523</td>\n",
       "      <td>0.381142</td>\n",
       "      <td>0.190069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ashlra638</td>\n",
       "      <td>ashley ray</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>reject</td>\n",
       "      <td>cultural fit</td>\n",
       "      <td>data scientist interview transcript \\n\\n compa...</td>\n",
       "      <td>sample resume ashley ray applying data scienti...</td>\n",
       "      <td>looking skilled data scientist expertise sql m...</td>\n",
       "      <td>306</td>\n",
       "      <td>2187</td>\n",
       "      <td>299</td>\n",
       "      <td>2632</td>\n",
       "      <td>0.292945</td>\n",
       "      <td>0.334349</td>\n",
       "      <td>0.072062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID          Name               Role decision Reason for decision  \\\n",
       "0  brenbr359   brent brown    product manager   select          experience   \n",
       "1  jameay305   james ayala  software engineer   select          experience   \n",
       "2  scotri565  scott rivera      data engineer   reject          experience   \n",
       "3  emilke232   emily kelly        ui engineer   select          experience   \n",
       "4  ashlra638    ashley ray     data scientist   reject        cultural fit   \n",
       "\n",
       "                                Transcript_processed  \\\n",
       "0  product manager interview transcript \\n\\n inte...   \n",
       "1  software engineer interview transcript \\n\\n in...   \n",
       "2  simulated interview scott rivera applying role...   \n",
       "3  interview transcript emily kelly ui engineer p...   \n",
       "4  data scientist interview transcript \\n\\n compa...   \n",
       "\n",
       "                                    Resume_processed  \\\n",
       "0  sample resume brent brown applying role produc...   \n",
       "1  sample resume james ayala applying software en...   \n",
       "2  sample resume scott rivera applying role data ...   \n",
       "3  sample resume emily kelly \\n\\n emily kelly \\n ...   \n",
       "4  sample resume ashley ray applying data scienti...   \n",
       "\n",
       "                           Job_Description_processed  Transcript_words  \\\n",
       "0  looking skilled product manager expertise data...               348   \n",
       "1  looking skilled software engineer expertise py...               387   \n",
       "2  looking skilled data engineer expertise cloud ...               282   \n",
       "3  looking skilled ui engineer expertise uiux des...               402   \n",
       "4  looking skilled data scientist expertise sql m...               306   \n",
       "\n",
       "   Transcript_length  Resume_words  Resume_length  Resume_JD  \\\n",
       "0               2537           221           2040   0.351376   \n",
       "1               2974           241           2231   0.272022   \n",
       "2               1989           255           2183   0.465514   \n",
       "3               2836           237           2035   0.313523   \n",
       "4               2187           299           2632   0.292945   \n",
       "\n",
       "   Resume_Transcript  JobDescription_Transcript  \n",
       "0           0.389509                   0.151835  \n",
       "1           0.380939                   0.091687  \n",
       "2           0.384318                   0.077372  \n",
       "3           0.381142                   0.190069  \n",
       "4           0.334349                   0.072062  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decision\n",
       "reject    0.403343\n",
       "select    0.385507\n",
       "Name: Resume_Transcript, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.groupby(['decision'])['Resume_Transcript'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decision\n",
       "reject    0.223339\n",
       "select    0.219486\n",
       "Name: Resume_JD, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.groupby(['decision'])['Resume_JD'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decision\n",
       "reject    0.096208\n",
       "select    0.116458\n",
       "Name: JobDescription_Transcript, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.groupby(['decision'])['JobDescription_Transcript'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preprocess the transcripts\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Preprocess all transcripts\n",
    "processed_transcripts = [preprocess_text(transcript) for transcript in new_df['Transcript_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=processed_transcripts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model for future use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec_transcript_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Vectorizing a single transcript\n",
    "def vectorize_transcript(transcript, model):\n",
    "    tokens = preprocess_text(transcript)\n",
    "    # Get the word vectors for each word in the transcript and average them\n",
    "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if word_vectors:\n",
    "        return sum(word_vectors) / len(word_vectors)\n",
    "    else:\n",
    "        return None  # If no words found in the model\n",
    "\n",
    "\n",
    "# Example: You can also vectorize all transcripts at once\n",
    "Transcript_vectors = np.array([vectorize_transcript(transcript, model) for transcript in new_df['Transcript_processed']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_resumes =  [preprocess_text(resume) for resume in new_df['Resume_processed']]\n",
    "processed_JD =  [preprocess_text(jd) for jd in new_df['Job_Description_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 =  Word2Vec(sentences=processed_resumes, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model2 = Word2Vec(sentences=processed_JD, vector_size=100, window=5, min_count=1,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resume_vectors = np.array([vectorize_transcript(resume, model1) for resume in new_df['Resume_processed']])\n",
    "JD_vectors = np.array([vectorize_transcript(jd, model2) for jd in new_df['Job_Description_processed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(\"word2vec_resume_model\")\n",
    "model2.save(\"word2vec_job_description_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tr_simi_w2v = [] \n",
    "re_jd_simi_w2v = []\n",
    "tr_jd_simi_w2v = [] \n",
    "\n",
    "for i in range(new_df.shape[0]):\n",
    "    re_tr_simi_w2v.append(cosine_similarity(Resume_vectors[i].reshape(-1,1),Transcript_vectors[i].reshape(-1,1))[0][0])\n",
    "    re_jd_simi_w2v.append(cosine_similarity(Resume_vectors[i].reshape(-1,1),JD_vectors[i].reshape(-1,1))[0][0]) \n",
    "    tr_jd_simi_w2v.append(cosine_similarity(Transcript_vectors[i].reshape(-1,1),JD_vectors[i].reshape(-1,1))[0][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Resume_Transcript_W2V'] = re_tr_simi_w2v \n",
    "new_df['Resume_JD_W2V'] = re_jd_simi_w2v \n",
    "new_df['JobDescription_Transcript_W2V'] = tr_jd_simi_w2v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decision\n",
       "reject   -0.352594\n",
       "select   -0.120557\n",
       "Name: Resume_Transcript_W2V, dtype: float32"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.groupby(['decision'])['Resume_Transcript_W2V'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decision\n",
       "reject    0.248851\n",
       "select    0.344948\n",
       "Name: Resume_JD_W2V, dtype: float32"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.groupby(['decision'])['Resume_JD_W2V'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decision\n",
       "reject   -0.294813\n",
       "select   -0.190244\n",
       "Name: JobDescription_Transcript_W2V, dtype: float32"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.groupby(['decision'])['JobDescription_Transcript_W2V'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\InterviewAutomation\\Automation\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Can't connect to HTTPS URL because the SSL module is not available.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load pre-trained model and tokenizer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# You can replace this with other models\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:858\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 858\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    860\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    689\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 690\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\huggingface_hub\\file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    858\u001b[0m     )\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\huggingface_hub\\file_download.py:923\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[1;32m--> 923\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\huggingface_hub\\file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\huggingface_hub\\file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1291\u001b[0m hf_headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1303\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\huggingface_hub\\file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    279\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    280\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    281\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\huggingface_hub\\file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    302\u001b[0m hf_raise_for_status(response)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\urllib3\\connectionpool.py:766\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;66;03m# Request a connection from the queue.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     timeout_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_timeout(timeout)\n\u001b[1;32m--> 766\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m     conn\u001b[38;5;241m.\u001b[39mtimeout \u001b[38;5;241m=\u001b[39m timeout_obj\u001b[38;5;241m.\u001b[39mconnect_timeout  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;66;03m# Is this a closed/new connection that requires CONNECT tunnelling?\u001b[39;00m\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\urllib3\\connectionpool.py:292\u001b[0m, in \u001b[0;36mHTTPConnectionPool._get_conn\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    289\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResetting dropped connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost)\n\u001b[0;32m    290\u001b[0m     conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\InterviewAutomation\\Automation\\lib\\site-packages\\urllib3\\connectionpool.py:1057\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1049\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting new HTTPS connection (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_connections,\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost,\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m443\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1054\u001b[0m )\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConnectionCls \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConnectionCls \u001b[38;5;129;01mis\u001b[39;00m DummyConnection:  \u001b[38;5;66;03m# type: ignore[comparison-overlap]\u001b[39;00m\n\u001b[1;32m-> 1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt connect to HTTPS URL because the SSL module is not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1059\u001b[0m     )\n\u001b[0;32m   1061\u001b[0m actual_host: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m   1062\u001b[0m actual_port \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport\n",
      "\u001b[1;31mImportError\u001b[0m: Can't connect to HTTPS URL because the SSL module is not available."
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"  # You can replace this with other models\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Automation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
